---
id: 50df57f0-bcfc-4ac4-8f5c-5eac660cdf7f
title: 'Building Modular Monoliths in Lumen'
subtitle: 'This could be considered Part 2 in an evolving series about building an API in Laravel’s Lumen micro-framework. Please feel free to read…'
published: false
original: medium
---



### Building Modular Monoliths in Lumen

*This could be considered* ***Part 2*** *in an evolving series about building an API in Laravel’s Lumen micro-framework. Please feel free to read* [*Part 1*](https://medium.com/@simonhamp/a-modular-monolith-in-laravel-lumen-4933b8cab2f1) *at your leisure.*

### Some challenges!

Composing what would otherwise be a single application from multiple disparate pieces that are co-dependent does introduce some interesting headaches.

Perhaps the first problem is how to keep code in the main application (in our case, the API shell) up to date as painlessly as possible when it consists of separate projects (Composer packages) that are being frequently updated.

In a traditional Composer setup, this is a bit tricky because packages are installed by copying the contents of the Git repositories into your application, checking them out under `/vendor`. Any updates are usually handled manually using `composer update`.

If we were to use this approach for our Suite packages, we have to do something like this:

- Make change in package code
- Commit changes (&amp; merge if they’re on a branch &amp; tag if we’re not using `dev-master`)
- Push changes to the central repo (e.g. GitHub)
- Run `composer update` in the main app

If you’ve ever done this, you’ll know that most of these steps can take many seconds to minutes. This adds up quickly.

Plus, when your main application *and* its dependencies are being developed on the same machine, it just feels really redundant to go through this whole flow every time you want to test a code change.

Perhaps we’re able to save some of this time? Yep, we sure can, thanks to Composer. Here are a couple of ways:

**NB: Before you can do anything here, make sure your packages have at least** [**a valid** ](https://getcomposer.org/doc/04-schema.md)[`composer.json`**file**](https://getcomposer.org/doc/04-schema.md) **in their root.**

#### Path repository

This method is the quickest to get going with because you don’t need to set your package up in Git or push to a central repo (even though you should); you can just start using your package as a dependency straight away.

Simply add it as a [`path`repository](https://getcomposer.org/doc/05-repositories.md#path) in your application’s `composer.json`:

```
"require": {
    "super-dev/module-package-1": "dev-master"
},
"repositories": [
    {
        "type": "path",
        "url": "../relative/path/to/module-package-1"
    }
]
```

Then run `composer install`. You should see the package gets **symlinked**. This means you don’t need to keep running `composer update` every time you make a change in your package.

This is ok for a while, but as soon as you want to deploy to staging or production, you’ll need to change this, either by changing the path repository into a [`vcs`repository](https://getcomposer.org/doc/05-repositories.md#loading-a-package-from-a-vcs-repository) (or `composer` repository for Private Packagist or Satis) or removing it altogether. (Composer doesn’t allow falling back on different sources for a package, which is an annoyance when you’re developing locally *and* deploying to production.)

One problem with this is that `path` repository info is stored in your `composer.lock` file too, which means this can’t really be shared well with others.

Basically, I wouldn’t recommend committing the `path` repository stuff to your application’s Git repo as it’s probably not very useful outside of the context of your computer. If you do want to keep it around, make sure it’s on a local-only branch.

This leads me to my preferred approach… (Until a better solution comes along)

#### Override PSR-4 Autoloading

It goes a bit like this:

- Get the package set up in Git
- `require` it in your applicartion
- Then override the autoloading of its classes in your application’s `composer.json`

Here’s how:

```
"require": {
    "super-dev/module-package-1": "dev-master"
},
"autoload-dev": {
    "psr-4": {
        "SuperDev\\ModulePackageOne\\": "../modules/module-package-1/src/"
    }
}
```

This is a bit more dirty than the previous option, it requires a bit more set up (i.e. making sure your package is using PSR-4 autoloading) and it won’t work for everything (basically any files that aren’t PHP classes).

One thing you’ll notice locally is a bunch of warnings from Composer about conflicting classes, which will grow and grow as your package and class numbers increase.

Also, you might still need to remove some code for sharing and deployment (but I try to mitigate this by putting it in `autoload-dev`). But still it’s less intrusive, in my opinion.

As for setup, the very least you’ll need is your package set up in Git. Additionally, you may want it available via a Composer server — either Packagist, Private Packagist, or Satis — or just refer to it using a `vcs` repo.

Then when you `require` it, whatever is in Git will be what ends up in your `/vendor` folder. But when Composer generates the autoloader, it will use your development version of the package instead!

If your modules are primarily PHP classes, then it’s ok and you might find this easier to work with as it doesn’t change the `composer.lock` file.





---



When you’ve got some common base classes taking care of a lot of functionality, you end up with very sparse child classes. This is fine, but it also breeds ambiguity.

One thing we absolutely cannot have within our API is ambiguity! If I’m calling one endpoint that talks to a users table in the ChewitSuite, I cannot have it inadvertently fetching users from the StarburstSuite!

Equally, when working inside an abstract base class method that needs to know information available only to an instance of a concrete child class, context is key. Knowing what database to use, for example, and how to communicate with other classes in the same package is critical.

This highlighted something that I’d never had to consider before: **It’s basically impossible in the context of one package to reliably determine the context of something within another package**. This is one of the problems with complex packages that inter-depend on each other.

Let’s look at an example of this.

Let’s say you’ve created a package which has a Service Provider (`src/ServiceProvider.php`) which loads all of the config and services for the package.

Now in one of that package’s models you want to fetch a specific config key that’s available to that package’s config. Config keys are all namespaced by the name — something like `config(‘my_package.config_key’)` – should be easy, right?

We can reference this easily in the package’s models using the above code. But imagine for some reason we have to change the package name (who knows why, it could happen). So we’d have to change that reference everywhere. Not too bad if we only use it in one model in this one package, but trickier if it’s used elsewhere…

It gets worse if you want to get a *different* package’s config value… you can probably imagine where this is going to end up! In a right old mess of knowing which versions work with each other.

Basically we need an entry point to each packagd so that we can a well-defined method that will get the relevant value for us always in the correct context.

So, imagine something along the lines of `$this->package->config(‘config_key’)` inside the model instead.

The Service Provider gives us a suitable place for this kind of functionality. And if our package’s service provider and models are all in the same folder it’s easy. We can just reference our `ServiceProvider` class in our other classes and we’re done.

Just to be difficult though, let’s be an organised developer and put our models in a suitable subfolder and PSR-4 namespace `src/Models/Model.php`, `\\Package\\Models\\Model`.

Now, you can’t do a reverse relative namespace call ([yet](https://bugs.php.net/bug.php?id=52504)) in PHP, like `..\ServiceProvider`. So you have to use the full namespace name just to go up one level.

That’s a bit of a pain and it’s going to be a lot of duplication again in our code, but not insurmountable within a single package. That’s ok.

But now imagine that we’re in a shared linbrary package (a Support package) and it’s in this *Support package’s base model class* that we need to know the current child model class’s (in the other package) ServiceProvider to call some method available from it.

We can’t import ALL of the Service Provider classes in there for any and all packages we might ever have. It would get out of control!

And we can’t have references to all of the different package-specific versions of that one config key, that would be impossible.

To make matters worse, we’re not in a package’s model where we are quite close to a known Service Provider. **We’re in a completely different package** that needs to be able to speak to *any* package’s Service Provider dynamically!

Try and get your head around that one for a bit. It’s trickier than it may at first seem!





---



You could argue that it’s a shortcoming of PHP’s namespacing that it doesn’t allow relative calls to parent namespaces. So you must somehow always go back to a common root.

Doing that for a dynamic and basically unknowable namespace is quite tricky and super meta, but this is something we had to tackle because the alternative is to write lots of boilerplate code for each package that is basically identical in every case meaning there would be a lot of repetitive code knocking around.

This isn’t just dangerous because of throwing DRY out the window, it’s also a tempting situation for some ‘overzealous optimisation’ at some future point when you forget why the separate files are necessary, perform some hasty ‘refactor’ and then everything breaks.

So enabling this sort of deep package context all whilst adding very minimal mental burden on the developer is a real trick and I think we nailed.

We basically create a small manager class that knows about all of these related packages. When we register a package, it automatically gets added to a list of specific service providers that we keep a known reference to: our entry point to everything about a specific package.

The SuiteManager, as it’s called in our case, keeps tabs on all of these service providers and makes them accessible from anything that has access to it. So throw an instance of the in your DI container and now you can get information from any package really easily from anywhere in your application.

#### Authenticate Me

Authentication is probably one of the trickiest parts of our setup. It looks straightforward on paper, but when you get into it, you soon realise it has many unexpected complications.

Because we decided (and not lightly!) to use a *separate* OAuth server, and we want a simple registration flow from a user perspective when they use our mobile apps, we have a few challenges to overcome.

Registering users and granting OAuth tokens was a breeze thanks to Laravel Passport. We managed to get about 80% of that part set up in just a few days.

But there are some considerations when you’re using those tokens with Lumen:

1. You **MUST** validate any tokens used against your API. Remote token validation (aka [Token Introspection](https://tools.ietf.org/html/rfc7662)) isn’t natively supported by Passport yet. And besides, how do you do this without drastically affecting your APIs performance or breaking the trust around your tokens?
2. How the heck do we use those sexy scope middlewares now?
3. How on earth should we handle new user registrations!?

#### Validating Tokens

For the first question, really the only answer is to pass the token onto the OAuth server *as if* it was the API and see what it returns. A good endpoint to create for this is probably one that returns some details about the user of that token as that info will probably come in handy inside your API at some point.

The tricky part is making sure this doesn’t hit you on performance! The thing is we’re basically making our API (Lumen) a client of our OAuth server (Laravel) and the simplest transport to use is HTTP.

The HTTP method is fine, but **doing it on every request seems a bit wasteful**. In reality we only need to validate that token every so often to make sure it’s still usable.

Given our typical workload, most requests to our API happen within a small window of time, so **caching the response from our OAuth server for a period** is a viable way to limit the number of secondary HTTP requests we have to make and will keep our API feeling super snappy on 99% of requests.

As long as that validation time is a lot less than our standard token lifetime, we’re not unexpectedly granting anyone any extra time with a token that we know to have expired. In fact, we can still add expiration checking directly into Lumen, so even caching the token doesn’t pose a threat… effectively we’re only **caching the validity** of the token (and the credentials of the user in our case — encrypted of course!).

For sure there are still things that need to be carefully considered with this and we definitely haven’t thought of everything yet, but this works for now and those precious saved seconds and cycles all add up! Saving the planet, one HTTP request at a time. 👨🏻‍💻

#### Employing Scope Middleware

This one was actually quite straightforward. The scope middlewares that come with Passport are simple self-contained files, so as long as we’re returning the user’s scopes in our token validation step somehow, then we can use them with the middleware. We basically just need to bring those middleware files into Lumen.

#### User Registration

As for user registration, this is a little more complex. In order for one of our users to successfully authenticate and make requests to the API, they now need to be registered on our OAuth service.

But we don’t want any customers to ever really ‘see’ that service. They will mostly be using our apps. The user ‘delegates’ access to their account to the app and the app uses that delegation to speak to the API on their behalf.

We went with OAuth because it’s a tried and tested solution that supports all of our use cases almost perfectly. It’s going to support users across all sorts of systems, the same user on multiple systems and also all of our internal users.

So when a new customer signs up using one app, the app needs register the details the user gives it (username, password etc) and also create an associated account in the relevant API Suite for that client application.

Internally, we register the user on the OAuth service first and then in the relevant Suite database. Once that’s done, the app can use the same login details to get an OAuth access token. At which point, it can basically throw away those login credentials.

So client apps never need to store a username and password and we’re keeping only one central set of secure login credentials well away from any Personally Identifiable Information and also making it quite tricky to associate any in-app data with a specific person.

Another benefit comes from short token lifetimes. Our access tokens last for less than 48 hours. This forces the app to use its refresh token, effectively re-authenticating every couple of days, further protecting the user’s data.

#### Databases &amp; Migrations

Another difficult aspect we needed to crack was that of data access and database migrations.

This was actually something I tried to tackle very early on as I had a decent plan in mind to make something that was both flexible and stayed true to the configurability of Laravel.

**The challenges here are many, but the primary one is definitely ease of on-boarding new developers to the project. If we can get them set up simply by cloning a couple of repos and running a couple of Artisan commands, that’s probably a great start.**

Because everyone’s setup is different, I was keen to try and make everything work flexibly from the `.env` file, so that everything could be easily configured to a very high level of granularity (i.e. per-suite database configuration) for more complex setups – particularly in production – and then kept fairly simplified for more standard local setups.

The approach I came up with is very specific to our production environment, but it’s flexible enough to stick everything in a single database locally and still work.

In production, *it could be* that each Suite’s database(s) lives on their own server. At the very least, each Suite will have its own separate database even if they were all on the same server. This is so that we can have tight controls around access to specific databases: there is one MySQL user for each database that can only perform basic CRUD actions on tables in *their database*, no more.

Further, migration actions are handled by a migrator user which again has limited and specific access to allow/restrict it to performing its duties.

We need configurable settings for all of these different users and databases, configuration that should never appear in version control and can be easily modified on a per-host basis. 🙀

The `.env` file is a perfect place for these, but Laravel’s default configuration setup doesn’t allow for this kind of variety of database connections. So we have to extend it with dynamic configuration that looks for `.env` keys and reasonable fallbacks if they don’t exist.

On top of that, **we also need to make sure that a Suite’s migrations use the correct migrator connection details just for the appropriate database**. Again, I wanted to do this in a way that doesn’t add any extra mental burden to writing migrations so we could stay as true to Laravel as possible for the ultimate convenience.

We came up with solutions for all of this! And I think it all works really really well. I’d love to go into more detail on any of these specific points if anyone’s interested…

#### Development

One of the things I actually appreciate in Laravel and that I have to admit I do miss in Lumen is the Artisan generators. There are some for Lumen, but their not as extensive as Laravel’s.

Even if we had Laravel’s generators though, they still wouldn’t really be enough. Most of the classes we’ll need to generate need to conform to a more specific structure and Laravel’s generated classes are a pretty basic starting point — we would still have a lot of work to do to get them working as Suite assets.

> It would be an absolute dream to be able to generate Suite-specific classes easily from a CLI command.

Thankfully we were able to adapt an [open-source implementation](https://github.com/flipboxstudio/lumen-generator) that someone else has kindly built. Bundling a customised version of Artisan along with our custom version of this generators package, each Suite (although it’s only a package) contains its own Artisan command with a bunch of tooling to make generating Models, Controllers, Tests and more super easy.

This makes for super rapid Suite development. In most cases we don’t even need to change *any* code in a generated Controller!

#### Exception Handling

One of our early design decisions was to respect (as closely as possible) principles from the JSON API standard. One of the aspects of this that I particularly like is the detailed error objects that get returned when something bad happens.

Thanks to Laravel’s excellent built-in exception handling, doing this is a cinch! Basically, we end up converting all unhandled exceptions into a new `JsonApiException` that can easily be rendered as a JSON object matching the JSON API spec.

What I’m particularly proud of with this solution is that it can actually handle a complete `ValidationException`, outputting each contained validation error as its own error in the output `errors` array.

We also have a very neat way of converting specific Exception classes to codes that we can reference in our documentation, linking developers directly to the point in the docs that explains the circumstances around that particular exception. Hopefully this will go a long way to help solving specific issues if/when we allow 3rd parties to access our API. Self-serving FTW!!! 🙌

#### Testing

I never used to test my code — not in a formalised, automated way. PHPUnit was always beyond me. When I finally made a serious attempt to get it into my workflow ([thanks Adam Wathan!](https://course.testdrivenlaravel.com/)), I realised how important it was and kicked myself for not doing it sooner.

But testing still isn’t easy. And it gets even harder when you’re building packages full of abstract classes or with migrations and controllers but basically no database until it gets loaded into an application.

So as you can imagine there are some unique challenges here too. The aim as always is to have as much of our code tested as possible. It’s also critical that tests happen as near to the code as they can — we wouldn’t want to be testing something in our Support package from within the API shell application.

We’ve managed to create an awesome rig for testing our Suites. It basically creates a Lumen application environment around each piece which provides a familiar testing platform.

It also has the side-effect of guaranteeing that what works in testing will work when it gets loaded into the API shell. Sweet!





---



Whew! We covered a lot here. That’s probably more than enough for now. I’ve got a ton more to talk about though. I’ll be saving this for Part 2, so come back again soon if you want to read about some of the following aspects of this approach and I’ll try to round everything off with why I feel this is such a beneficial way of building an API.

#### Versioning

#### Deployment

#### Documentation

#### Other neat tricks

- OAuth Hashed Client IDs
- Users and applications cannot be registered through the web interface of the OAuth server (CLI or API — only from registered client apps with a specific scope)
- Input and output handling
